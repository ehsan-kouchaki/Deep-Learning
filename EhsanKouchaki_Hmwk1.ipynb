{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ehsan Kouchaki\n",
    "**In this homework, you will learn how to:**\n",
    "- Implement a 2-class classification neural network \n",
    "- Use units with a non-linear activation function, such as tanh \n",
    "- Compute the cross entropy loss \n",
    "- Implement forward and backward propagation\n",
    "- Update weights using gradient descent\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "filename = 'cardio_dataset.csv'\n",
    "    \n",
    "reader = csv.reader(open(\"cardio_dataset.csv\", \"r\"), delimiter=\";\")\n",
    "x = list(reader)\n",
    "\n",
    "dataset_1 = np.array(x).astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now implement your model.\n",
    "<br>\n",
    "Please add cells and explain yours developing steps and your results.\n",
    "\n",
    "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\n",
    "برای توضیحات از زبان فارسی استفاده نمایید.\n",
    "<br>\n",
    "موفق باشید\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این بخش نخست داده ها به دو بخش داده های آموزشی و داده های تست تقسیم شده است. به این منظور 60000 داده نخست به عنوان داده های آموزشی و 10000 داده آخر به عنوان داده های تست در نظر گرفته شدند. پس از آن داده ها اصلاح شده است. به این صورت که نخست داده ها ترانهاده شده و پس از آن سطر اول داده ها که مربوط به شماره ردیف آنها است حذف شده است. به این ترتیب همانند مواردی که در کلاس درس گفته شد ماتریس داده هابه این صورت تشکیل شد که در هر ستون ویژگی های یک داده قرار داده شده است. به این ترتیب تعداد ردیف های ماتریس داده ها برابر با تعداد ویژگی ها و تعداد ستون های آن برابر با تعداد نمونه های آموزشی شده است همچنین ردیف آخر که مربوط به برچسب داده ها است با نام جداگانه به عنوان بردار برچسب جدا سازی و نامگذاری شده است"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train data is:  (11, 60000)\n",
      "The shape of test data is:  (11, 10000)\n",
      "The shape of train ground truth is:  (60000,)\n",
      "The shape of test ground truth is:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "trains_number = 60000\n",
    "dataset_train = dataset_1[0:trains_number].T\n",
    "dataset_test = dataset_1[trains_number:70000].T\n",
    "\n",
    "X_train = dataset_train[1:12]    # train data\n",
    "X_test = dataset_test[1:12]      # test data\n",
    "\n",
    "Y_train = dataset_train[12]        # train ground truth\n",
    "Y_test = dataset_test[12]          # test ground truth\n",
    "\n",
    "print(\"The shape of train data is: \", X_train.shape)\n",
    "print(\"The shape of test data is: \", X_test.shape)\n",
    "print(\"The shape of train ground truth is: \", Y_train.shape)\n",
    "print(\"The shape of test ground truth is: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization\n",
    "<div dir=\"rtl\">\n",
    "با توجه به اینکه ویژگیهای داده ها پراکندگی زیادی دارند، در این بخش داده ها با استفاده از واریانس میانگین که در کلاس درس گفته شد نرمال سازی شده اند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    mean_train = np.mean(X_train[i])\n",
    "    X_train[i] -= mean_train\n",
    "    mean_test = np.mean(X_test[i])\n",
    "    X_test[i] -= mean_test\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    var_train = np.var(X_train[i])\n",
    "    X_train[i] /= var_train\n",
    "    var_test = np.var(X_test[i])\n",
    "    X_test[i] /= var_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "<div dir=\"rtl\">\n",
    "در این بخش توابع فعال ساز رلو و سیگمویید برای استفاده تعریف شده اند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma = 1/(1 + np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "def dsigmoid(z):\n",
    "    sigma = 1/(1 + np.exp(-z))\n",
    "    sigmap = sigma * (1 - sigma)\n",
    "    return sigmap\n",
    "\n",
    "def relu(z):\n",
    "    rel = np.maximum(0, z)\n",
    "    return rel\n",
    "\n",
    "def drelu(z):\n",
    "    z[z<=0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine and activation modules\n",
    "<div dir=\"rtl\">\n",
    "در این بخش تابعهایی برای ماژولهای مورد استفاده برای ساختن شبکه عصبی نوشته شده است"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def Relu_forward(Z):\n",
    "    A = relu(Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def Sigmoid_forward(Z):\n",
    "    A = sigmoid(Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def compute_cost(Y_hat, Y):\n",
    "    cost = - np.sum(np.multiply(np.log(Y_hat),Y) + np.multiply(np.log(1 - Y_hat),(1 - Y))) / Y_hat.shape[1]\n",
    "    return cost\n",
    "\n",
    "def affine_backward(dZ, cache):\n",
    "    A_l_1, W_l, b_l = cache[0], cache[1], cache[2]\n",
    "    m = A_l_1.shape[1]\n",
    "    \n",
    "    dW_l = 1 / m * np.dot(dZ, A_l_1.T)\n",
    "    db_l = 1 / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_l_1 = np.dot(W_l.T, dZ)\n",
    "    return dA_l_1, dW_l, db_l\n",
    "\n",
    "def Relu_backward(dA, cache):\n",
    "    Z_l = cache\n",
    "    dZ = dA * drelu(Z_l)\n",
    "    return dZ\n",
    "\n",
    "def Sigmoid_backward(dA, cache):\n",
    "    Z_l = cache\n",
    "    dZ = dA * dsigmoid(Z_l)\n",
    "    return dZ\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network design \n",
    "<div dir=\"rtl\">\n",
    "در این بخش شبکه عصبی با مشخص کردن تعداد لایه ها و تعداد نرون ها در هر لایه مخفی طراحی شده است. برنامه این قابلیت را دارد که در همین قسمت به تعداد دلخواه لایه ها و تعداد نرون ها انتخاب و تعیین شود "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "همه لایه ها و تعداد نرون ها شامل لایه ورودی و خروجی:\n",
      "\n",
      "  [11, 10, 5, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "Layers = [10, 5, 3]    # تعداد لایه ها مخفی به همراه تعدادنرون ها در هر لایه \n",
    "Layers.insert(0,X_train.shape[0])\n",
    "Layers.append(1)\n",
    "print(\"همه لایه ها و تعداد نرون ها شامل لایه ورودی و خروجی:\\n\\n \", Layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train \n",
    "<div dir=\"rtl\">\n",
    "در این بخش آموزش شبکه عصبی نوشته شده است\n",
    "بر اساس توضیحات ارایه شده در کلاس این بخش به صورت ماژولار و با استفاده از بلوکهای جداگانه برای محاسبات به جلو و پس انتشار خطا و ذخیره خروجی های مورد نیاز به صورت مستقیم و در قالب کش نوشته شده است"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d90f0c062e29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# انتشار به جلو برای لایه های مخفی\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b1c9a97223c8>\u001b[0m in \u001b[0;36maffine_forward\u001b[0;34m(A, W, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maffine_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# مقدار دهی اولیه وزنها و بایاس ها\n",
    "Weights = []                  # لیست نگهدارنده وزنها\n",
    "biases = []                   # لیست نگهدارنده بایاسها\n",
    "\n",
    "for l in range(1, len(Layers)):\n",
    "    # روش تصادفی\n",
    "    Weights.append(np.random.randn(Layers[l], Layers[l-1]) * 0.01)\n",
    "    # روش He\n",
    "    #Weights.append(np.random.normal(loc=0, scale=np.sqrt(2/Layers[l-1]), size=(Layers[l], Layers[l-1])))\n",
    "    \n",
    "    biases.append(np.zeros((Layers[l], 1)))\n",
    "    \n",
    "L = len(Weights)     # the number of layers\n",
    "\n",
    "losses = []\n",
    "Epochs = 30000\n",
    "for epoch in range(Epochs):\n",
    "    \n",
    "    learning_rate = 13 / (epoch + 1)\n",
    "    \n",
    "    A = X_train\n",
    "\n",
    "    caches = []\n",
    "    # انتشار به جلو برای لایه های مخفی\n",
    "    for l in range(L-1):\n",
    "        Z, cache = affine_forward(A, Weights[l], biases[l])\n",
    "        caches.append(cache)\n",
    "        A, cache = Relu_forward(Z)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # انتشار به جلو برای لایه آخر\n",
    "    Z, cache = affine_forward(A, Weights[L-1], biases[L-1])\n",
    "    caches.append(cache)\n",
    "    Y_hat , cache = Sigmoid_forward(Z)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # محاسبه هزینه\n",
    "    #Y_train = Y_train.reshape(Y_hat.shape)    # با این کار دو بردار هم اندازه می شود\n",
    "    Y_hat += 0.0000001*(Y_hat == 0) - 0.0000001*(Y_hat == 1)    #  اطمینلن از اینکه در تابع هزینه از صفر لگاریتم گرفته نشود\n",
    "    cost = compute_cost(Y_hat, Y_train)\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # پس انتشار برای لایه آخر\n",
    "    current_cache = caches[2 * L - 1]   # Z of the last layer\n",
    "    dZ = Y_hat-Y_train\n",
    "    #dZ = Sigmoid_backward(dA, current_cache)\n",
    "    current_cache = caches[2 * L - 2]\n",
    "    dA, dW, db = affine_backward(dZ, current_cache)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = dA, dW, db\n",
    "\n",
    "    # پس انتشار برای لایه های مخفی\n",
    "    for l in range(L-2,-1,-1):\n",
    "        current_cache = caches[2 * l + 1]   # Z of the l'th layer\n",
    "        dZ =Relu_backward(dA, current_cache)\n",
    "        current_cache = caches[2 * l]\n",
    "        dA, dW, db = affine_backward(dZ, current_cache)\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = dA, dW, db\n",
    "\n",
    "    # بروز کردن وزنها و بایاسها\n",
    "    for l in range(L):\n",
    "        Weights[l] = Weights[l] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        biases[l] = biases[l] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    # Print the cost\n",
    "    if epoch % int(Epochs / 20) == 0:\n",
    "        print (\"Cost after iteration %i: %f\" %(epoch, cost))\n",
    "    \n",
    "    losses.append(cost)\n",
    "    \n",
    "plt.plot(losses)\n",
    "\n",
    "predictions = Y_hat > 0.5   # خروجی های بزرگتر از نیم به عنوان 1 در نظر گرفته می شوند\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))\n",
    "\n",
    "# Accuracy\n",
    "print ('Accuracy: %d' % float((np.dot(Y_train,predictions.T)\n",
    "                               + np.dot(1-Y_train,1-predictions.T))/float(Y_train.size)*100) + '%')\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions mean = 0.4944\n",
      "Accuracy: 71%\n"
     ]
    }
   ],
   "source": [
    "A = X_test\n",
    "for l in range(L-1):\n",
    "    Z, cache = affine_forward(A, Weights[l], biases[l])\n",
    "    A, cache = Relu_forward(Z)\n",
    "\n",
    "Z, cache = affine_forward(A, Weights[L-1], biases[L-1])\n",
    "Y_hat , cache = Sigmoid_forward(Z)\n",
    "\n",
    "predictions = Y_hat > 0.5\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))\n",
    "\n",
    "# Accuracy\n",
    "print ('Accuracy: %d' % float((np.dot(Y_test,predictions.T) + np.dot(1-Y_test,1-predictions.T))/float(Y_test.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Weights0.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Weights, fp)\n",
    "\n",
    "with open(\"Weights0.txt\", \"rb\") as fp:   # Unpickling\n",
    "    w = pickle.load(fp)\n",
    "\n",
    "with open(\"biases0.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(biases, fp)\n",
    "\n",
    "with open(\"biases0.txt\", \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)\n",
    "\n",
    "with open(\"losses0.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(losses, fp)\n",
    "\n",
    "with open(\"losses0.txt\", \"rb\") as fp:   # Unpickling\n",
    "    l = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    \n",
    "# جمع بندی و نتایج\n",
    "    در این تکلیف با استفاده از شبکه عصبی مساله دسته بندی افراد برای تشخیص بیماری قلبی با استفاده از داده های کاردیوگرافی حل شد.\n",
    " ##   ویژگی های مدل حاضر\n",
    "#### ۱- این مدل با استفاده از ماژولهای بیشرو و پس رو و ذخیره موارد مورد نیاز در انتشار به جلو به عنوان کش ساخته شد\n",
    "#### ۲-  این برنامه قابلیت تعیین شبکه با تعداد لایه های دلخواه و تعداد نرون ها را دارد بدون اینکه نیاز به نوشتن برنامه جدیدی برای هر حالت باشد\n",
    "#### ۳- در این برنامه وزدن دهی اولیه به روش هی نیز انجام شد.\n",
    "#### ۴- در این مدل از نرخ یادگیری متغیر که نسبت عکس با شماره ایپاک دارد استفاده شد تا علاوه بر افزایش سرعت همگرایی در گامهای اولیه در گامهای نهایی با کاهش نرخ یادگیری دقت همگرایی افزایش یابد\n",
    "#### \n",
    "    \n",
    "#  نتایج\n",
    "## تاثیر تعداد لایه ها\n",
    "#### در این تکلیف مساله با تعیین تعداد لایه های مختلف شبکه عصبی حل شد و هر بار نتایج مختلفی به دست آمد. با در نظر گرفتن شبکه به صورت ساده دارای یک یا دو لایه مخفی نتایج خوبی از نظر دقت به دست نیامد به عنوان نمونه در نظر نگرفتن لایه مخفی برای مساله شبکه را تبدیل به یک سیستم مانند لاجیستیک رگرسیون می کند و عملکرد ضعیفی خواهد داشت. از طرفی تعداد لایه های مخفی از 3 لایه بیشتر نه تنها باعث سنگینتر شدن روند حل و افزایش زمان حل شد بلکه تاثیر مثبتی در دقت شبکه ایجاد نکرد. بهترین نتیجه با در نظر گرفتن سه لایه مخفی برای این مساله به دست آمد\n",
    "## تاثیر تعداد نرونها\n",
    "#### برای حل این مساله تعداد مختلفی نرون برای لایه های مختلف شبکه در نظر گرفته شد با انتخاب تعداد نرون کم در لایه ها مثلا در حد ۳ یا چهار نرون شبکه از قدرت خوبی برخوردار نبود از طرفی تعداد نرون بیش از حد زیاد نیز تاثیری در عملکرد مثبت شبکه ایجاد  نکرد. درنهایت با انتخاب تعداد نرون ها به صورت مخروطی و با اعداد موجود در همین خروجی برنامه حاضر تقریبا بهترین پاسخها به دست آمد\n",
    "## تاثیر نوع مقدار دهی وزنها\n",
    "#### در این برنامه از دو روش تصادفی و روش هی برای مقدار دهی اولیه وزنها استفاده شد. در نهایت برای این مساله استفاده از روش   هی موجب ایجاد مقادیر وزنهای بزرگتری در ایپاکهای اولیه و همچنین مقدار هزینه بزرگتر شد ولی به تدریج مقادیر هزینه کاهش یافت. در نهایت استفاده از دو روش مقدار دهی تصادفی و روش هی تفاوت چندانی در دقت نهایی داده های آموزش و تست برای این مساله ایجاد نکرد\n",
    "## تاثیر تعداد داده های آموزشی\n",
    "#### در این برنامه مساله با تقسیم داده ها به آموزشی و تست برای چند حالت مختلف انجام شد در صورت انتخاب تعداد داده های آموزشی به تعداد کم شبکه به طور مطلوبی آموزش نمیدید از طرفی انتخاب بیش از حد معمول تعداد داده های آموزشی موجب بر هم خوردن تعادل نسبت بین داده های آموزشی و تست می گردد و موجب می شود معیار درستی برای دقت عملکرد شبکه به دست نیاید در این مساله داده های آموزشی به تعداد 60000 در نهایت نتایج مطلوبی به دست داد هرجند انتخاب داده ها به میزان کمتر و در حدود بین 40000 تا 60000 تفاوت چندانی در نتایج ایجاد نکرد    \n",
    "\n",
    "    \n",
    "# نتایج بهترین وزنها و بایاسهای به دست آمده ذخیره شده و به پیوست ارسال شده است \n",
    "    \n",
    "    \n",
    "    \n",
    "    <div dir=\"rtl\">\n",
    "    \n",
    "# جمع بندی و نتایج\n",
    "    در این تکلیف با استفاده از شبکه عصبی مساله دسته بندی افراد برای تشخیص بیماری قلبی با استفاده از داده های کاردیوگرافی حل شد.\n",
    " ##   ویژگی های مدل حاضر\n",
    "#### ۱- این مدل با استفاده از ماژولهای بیشرو و پس رو و ذخیره موارد مورد نیاز در انتشار به جلو به عنوان کش ساخته شد\n",
    "#### ۲-  این برنامه قابلیت تعیین شبکه با تعداد لایه های دلخواه و تعداد نرون ها را دارد بدون اینکه نیاز به نوشتن برنامه جدیدی برای هر حالت باشد\n",
    "#### ۳- در این برنامه وزدن دهی اولیه به روش هی نیز انجام شد.\n",
    "#### ۴- در این مدل از نرخ یادگیری متغیر که نسبت عکس با شماره ایپاک دارد استفاده شد تا علاوه بر افزایش سرعت همگرایی در گامهای اولیه در گامهای نهایی با کاهش نرخ یادگیری دقت همگرایی افزایش یابد\n",
    "#### \n",
    "    \n",
    "#  نتایج\n",
    "## تاثیر تعداد لایه ها\n",
    "#### در این تکلیف مساله با تعیین تعداد لایه های مختلف شبکه عصبی حل شد و هر بار نتایج مختلفی به دست آمد. با در نظر گرفتن شبکه به صورت ساده دارای یک یا دو لایه مخفی نتایج خوبی از نظر دقت به دست نیامد به عنوان نمونه در نظر نگرفتن لایه مخفی برای مساله شبکه را تبدیل به یک سیستم مانند لاجیستیک رگرسیون می کند و عملکرد ضعیفی خواهد داشت. از طرفی تعداد لایه های مخفی از 3 لایه بیشتر نه تنها باعث سنگینتر شدن روند حل و افزایش زمان حل شد بلکه تاثیر مثبتی در دقت شبکه ایجاد نکرد. بهترین نتیجه با در نظر گرفتن سه لایه مخفی برای این مساله به دست آمد\n",
    "## تاثیر تعداد نرونها\n",
    "#### برای حل این مساله تعداد مختلفی نرون برای لایه های مختلف شبکه در نظر گرفته شد با انتخاب تعداد نرون کم در لایه ها مثلا در حد ۳ یا چهار نرون شبکه از قدرت خوبی برخوردار نبود از طرفی تعداد نرون بیش از حد زیاد نیز تاثیری در عملکرد مثبت شبکه ایجاد  نکرد. درنهایت با انتخاب تعداد نرون ها به صورت مخروطی و با اعداد موجود در همین خروجی برنامه حاضر تقریبا بهترین پاسخها به دست آمد\n",
    "## تاثیر نوع مقدار دهی وزنها\n",
    "#### در این برنامه از دو روش تصادفی و روش هی برای مقدار دهی اولیه وزنها استفاده شد. در نهایت برای این مساله استفاده از روش   هی موجب ایجاد مقادیر وزنهای بزرگتری در ایپاکهای اولیه و همچنین مقدار هزینه بزرگتر شد ولی به تدریج مقادیر هزینه کاهش یافت. در نهایت استفاده از دو روش مقدار دهی تصادفی و روش هی تفاوت چندانی در دقت نهایی داده های آموزش و تست برای این مساله ایجاد نکرد\n",
    "## تاثیر تعداد داده های آموزشی\n",
    "#### در این برنامه مساله با تقسیم داده ها به آموزشی و تست برای چند حالت مختلف انجام شد در صورت انتخاب تعداد داده های آموزشی به تعداد کم شبکه به طور مطلوبی آموزش نمیدید از طرفی انتخاب بیش از حد معمول تعداد داده های آموزشی موجب بر هم خوردن تعادل نسبت بین داده های آموزشی و تست می گردد و موجب می شود معیار درستی برای دقت عملکرد شبکه به دست نیاید در این مساله داده های آموزشی به تعداد 60000 در نهایت نتایج مطلوبی به دست داد هرجند انتخاب داده ها به میزان کمتر و در حدود بین 40000 تا 60000 تفاوت چندانی در نتایج ایجاد نکرد    \n",
    "\n",
    "    \n",
    "# نتایج بهترین وزنها و بایاسهای به دست آمده ذخیره شده و به پیوست ارسال شده است \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
